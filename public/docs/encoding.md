# Encodage des variables cat√©gorielles (Feature Encoding)

> üì¶ Pr√©traitement fondamental en Machine Learning & NLP

---

## 1. Probl√©matique

Les algorithmes de Machine Learning op√®rent sur des **vecteurs num√©riques** :

$$
x \in \mathbb{R}^n
$$

Or, de nombreux datasets contiennent des **variables cat√©gorielles** :

* pays : Cameroun, France, Canada
* couleur : rouge, vert, bleu
* texte : mots, sous-mots, tokens

Ces variables appartiennent √† un **ensemble discret fini** :

$$
x \in \mathcal{C} = {c_1, c_2, \dots, c_K}
$$

Impossible donc de les utiliser directement dans :

* distances (KNN)
* produits scalaires (r√©gression, r√©seaux)
* optimisations diff√©rentiables

---

## 1.1 Objectif de l‚Äôencodage

On cherche une application :

$$
\phi : \mathcal{C} \rightarrow \mathbb{R}^d
$$

transformant une cat√©gorie en **repr√©sentation num√©rique exploitable**.

Selon l‚Äôalgorithme choisi :

* $d = 1$ (Label encoding)
* $d = K$ (One-Hot)
* $d = 1$ (Target encoding)
* $d \gg K$ (BPE / NLP)

---

# 2. Label Encoding

## 2.1 Principe

On associe un **indice entier unique** √† chaque cat√©gorie.

$$
\phi(c_k) = k-1
$$

---

## 2.2 Formulation

Soit :

$$
\mathcal{C} = {c_1, \dots, c_K}
$$

On d√©finit :

$$
\phi(c_k) \in {0,1,\dots,K-1}
$$

---

## 2.3 Exemple

| Cat√©gorie | Encodage |
| --------- | -------- |
| Rouge     | 0        |
| Vert      | 1        |
| Bleu      | 2        |

---

## 2.4 Propri√©t√©s math√©matiques

La variable devient scalaire :

$$
x \in \mathbb{Z}
$$

Mais cela induit **un ordre artificiel** :

$$
0 < 1 < 2
$$

donc :

$$
d(0,2) > d(0,1)
$$

ce qui n‚Äôa **aucun sens s√©mantique**.

---

## 2.5 Avantages / Limites

### Avantages

* simple
* m√©moire minimale $O(1)$

### Limites

* faux ordre
* biais dans distances et r√©gressions

---

# 3. One-Hot Encoding

## 3.1 Principe

Chaque cat√©gorie devient un **vecteur binaire orthogonal**.

---

## 3.2 Formulation

$$
\phi(c_k) = e_k
$$

o√π $e_k$ est le vecteur canonique :

$$
e_k = (0,\dots,1,\dots,0)^\top
$$

---

## 3.3 Exemple

Pour $K=3$ :

$$
Rouge = (1,0,0)
$$
$$
Vert = (0,1,0)
$$
$$
Bleu = (0,0,1)
$$

---

## 3.4 Propri√©t√©s g√©om√©triques

Orthogonalit√© :

$$
e_i^\top e_j =
\begin{cases}
1 & i=j \
0 & i\neq j
\end{cases}
$$

Distances :

$$
|e_i - e_j|_2 = \sqrt{2}
$$

Toutes les cat√©gories sont **√©quidistantes**.

---

## 3.5 Complexit√©

Dimension :

$$
d = K
$$

M√©moire :

$$
O(mK)
$$

Probl√®me si $K$ grand.

---

## 3.6 Avantages / Limites

### Avantages

* pas d‚Äôordre
* robuste pour mod√®les lin√©aires

### Limites

* explosion dimensionnelle
* sparse matrix

---

# 4. Target Encoding

## 4.1 Principe

On remplace chaque cat√©gorie par **l‚Äôesp√©rance conditionnelle de la cible**.

---

## 4.2 Formulation

Pour une cat√©gorie $c$ :

$$
\boxed{
\phi(c) =
\mathbb{E}[y \mid x=c]
}
$$

Estimation empirique :

$$
\phi(c)=
\frac{1}{N_c}
\sum_{i : x^{(i)}=c} y^{(i)}
$$

---

## 4.3 Interpr√©tation statistique

C‚Äôest une estimation de :

$$
\hat{f}(c) \approx \mathbb{E}[y|c]
$$

Donc une **r√©gression locale bay√©sienne**.

---

## 4.4 R√©gularisation (smoothing)

Pour √©viter l‚Äôoverfitting :

$$
\phi(c)=
\frac{N_c \mu_c + \alpha \mu}{N_c + \alpha}
$$

o√π :

* $\mu_c$ = moyenne locale
* $\mu$ = moyenne globale
* $\alpha$ = param√®tre de lissage

---

## 4.5 Avantages / Limites

### Avantages

* faible dimension ($d=1$)
* tr√®s performant

### Limites

* fuite de donn√©es (data leakage)
* n√©cessite cross-validation

---

# 5. Byte Pair Encoding (BPE)

## 5.1 Contexte NLP

Dans le texte, l‚Äôespace cat√©goriel est immense :

$$
|\mathcal{V}| \gg 10^5
$$

Un One-Hot serait impossible.

---

## 5.2 Principe

BPE construit un **vocabulaire de sous-mots** par fusion it√©rative.

---

## 5.3 Algorithme

### √âtape 1 ‚Äî caract√®res initiaux

```
l o w
l o w e r
```

### √âtape 2 ‚Äî fusion paire fr√©quente

Si "lo" fr√©quent :

```
lo w
lo w e r
```

R√©p√©ter jusqu‚Äô√† taille voulue.

---

## 5.4 Formulation probabiliste

On cherche √† maximiser :

$$
\text{freq}(ab)
$$

Fusion :

$$
(a,b) \rightarrow ab
$$

Minimisant la longueur totale de description (compression).

---

## 5.5 Repr√©sentation vectorielle

Apr√®s tokenisation :

* soit One-Hot sur tokens
* soit embeddings :

$$
\phi(token) \in \mathbb{R}^d
$$

---

## 5.6 Avantages

* vocabulaire r√©duit
* g√®re mots inconnus
* base des LLM modernes

---

# 6. Comparaison th√©orique

| M√©thode | Dimension | Ordre artificiel | Risque overfit | Cas d‚Äôusage         |
| ------- | --------- | ---------------- | -------------- | ------------------- |
| Label   | 1         | Oui              | Faible         | arbres              |
| One-Hot | K         | Non              | Faible         | mod√®les lin√©aires   |
| Target  | 1         | Non              | √âlev√©          | tabulaire supervis√© |
| BPE     | variable  | Non              | Faible         | NLP                 |

---

# 7. Choix pratique

## Petit $K$

‚Üí One-Hot

## Grand $K$

‚Üí Target encoding

## NLP

‚Üí BPE

## Arbres (RandomForest, XGBoost)

‚Üí Label encoding acceptable

---

# 8. R√©sum√© math√©matique

### Mapping g√©n√©ral

$$
\phi : \mathcal{C} \rightarrow \mathbb{R}^d
$$

### Label

$$
\phi(c)=k
$$

### One-Hot

$$
\phi(c)=e_k
$$

### Target

$$
\phi(c)=\mathbb{E}[y|c]
$$

### BPE

$$
texte \rightarrow tokens \rightarrow embeddings
$$

---

# 9. Conclusion

L‚Äôencodage transforme un **espace symbolique discret** en **espace vectoriel continu**, condition n√©cessaire pour :

* distances
* produits scalaires
* optimisation

Le choix d√©pend :

* cardinalit√©
* type de mod√®le
* risque de sur-apprentissage
* nature des donn√©es (tabulaire vs texte)

Ainsi, l‚Äôencodage constitue une √©tape **math√©matiquement cruciale** du pipeline ML, au m√™me titre que la normalisation ou la r√©duction de dimension.
