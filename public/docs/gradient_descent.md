# Descente des Gradients ‚Äî Documentation Compl√®te

> üì¶ Projet source : [ml-from-scratch / regression_lineaire](https://github.com/Padevend/ml-from-scratch/tree/main/regression_lineaire)  
---

## 1. Intuition g√©om√©trique

### 1.1 Le probl√®me d'optimisation

Tout algorithme d'apprentissage se ram√®ne √† un probl√®me central : **trouver les param√®tres $\theta$ qui minimisent une fonction de co√ªt $J(\theta)$**.

$$\theta^* = \arg\min_{\theta} \; J(\theta)$$

La fonction $J$ d√©finit un **paysage en relief** (surface) dont les axes horizontaux sont les param√®tres $\theta$ et l'axe vertical est la valeur du co√ªt. Le minimum global est le point le plus bas de ce paysage.

### 1.2 La m√©taphore de la colline

Imaginez √™tre debout sur une colline dans le brouillard ‚Äî vous ne voyez pas le fond de la vall√©e. La seule information disponible est la **pente locale** sous vos pieds. La strat√©gie naturelle est :

1. Sentir la pente √† votre position actuelle
2. Faire un pas dans la direction **oppos√©e** √† la mont√©e (donc vers le bas)
3. R√©p√©ter

C'est exactement ce que fait la descente des gradients : √† chaque it√©ration, elle se d√©place dans la direction qui fait **diminuer $J$ le plus vite localement**.

### 1.3 R√¥le du gradient

Le gradient $\nabla_\theta J$ est un vecteur qui point dans la direction de **plus grande croissance** de $J$. On se d√©place donc dans la direction **oppos√©e** :

$$\boxed{\theta \leftarrow \theta - \alpha \, \nabla_\theta J(\theta)}$$

Le signe n√©gatif est crucial : sans lui, on monterait au lieu de descendre.

---

## 2. Formulation math√©matique

### 2.1 D√©finition du gradient

Soit $J : \mathbb{R}^{n+1} \to \mathbb{R}$ une fonction differentiable. Le gradient est le vecteur des d√©riv√©es partielles :

$$\nabla_\theta J = \begin{pmatrix} \dfrac{\partial J}{\partial \theta_0} \\[10pt] \dfrac{\partial J}{\partial \theta_1} \\[10pt] \vdots \\[6pt] \dfrac{\partial J}{\partial \theta_n} \end{pmatrix} \in \mathbb{R}^{n+1}$$

Chaque composante $\frac{\partial J}{\partial \theta_j}$ mesure comment $J$ change lorsqu'on perturbe uniquement $\theta_j$, en gardant tous les autres param√®tres fixes.

### 2.2 R√®gle de mise √† jour

√Ä chaque it√©ration $t$, on met √† jour **simultan√©ment** tous les param√®tres :

$$\theta^{(t+1)} = \theta^{(t)} - \alpha \, \nabla_\theta J\!\left(\theta^{(t)}\right)$$


### 2.3 D√©composition par param√®tre

La mise √† jour peut aussi s'√©crire composante par composante :

$$\theta_j^{(t+1)} = \theta_j^{(t)} - \alpha \, \frac{\partial J}{\partial \theta_j}\bigg|_{\theta = \theta^{(t)}}, \quad \forall\, j \in \{0, 1, \dots, n\}$$

---

## 3. Application √† la r√©gression lin√©aire

On reprend les notations du document `regression_lineaire.md`.

### 3.1 Fonction de co√ªt (MSE)

$$J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} \left(\theta^\top x^{(i)} - y^{(i)}\right)^2 = \frac{1}{2m} \|X\theta - Y\|_2^2$$

### 3.2 Calcul du gradient ‚Äî d√©rivation compl√®te

On d√©veloppe $J$ pour une composante $\theta_j$ quelconque :

$$\frac{\partial J}{\partial \theta_j} = \frac{\partial}{\partial \theta_j} \left[ \frac{1}{2m} \sum_{i=1}^{m} \left(\sum_{k=0}^{n} \theta_k x_k^{(i)} - y^{(i)}\right)^2 \right]$$

On applique la r√®gle de la cha√Æne $\frac{d}{dx}[u^2] = 2u \cdot \frac{du}{dx}$ :

$$= \frac{1}{2m} \sum_{i=1}^{m} 2\left(\sum_{k=0}^{n} \theta_k x_k^{(i)} - y^{(i)}\right) \cdot \frac{\partial}{\partial \theta_j}\left(\sum_{k=0}^{n} \theta_k x_k^{(i)}\right)$$

La d√©riv√©e partielle int√©rieure est imm√©diate : seul le terme $k = j$ survit :

$$\frac{\partial}{\partial \theta_j}\left(\sum_{k=0}^{n} \theta_k x_k^{(i)}\right) = x_j^{(i)}$$

On simplifie le facteur 2 avec $\frac{1}{2m}$ :

$$\boxed{\frac{\partial J}{\partial \theta_j} = \frac{1}{m} \sum_{i=1}^{m} \left(\theta^\top x^{(i)} - y^{(i)}\right) x_j^{(i)}}$$

### 3.3 Forme matricielle du gradient

En regroupant toutes les composantes simultan√©ment :

$$\nabla_\theta J = \frac{1}{m} X^\top (X\theta - Y)$$

**V√©rification des dimensions :**

| Matrice | Dimensions |
|---|---|
| $X^\top$ | $(n+1) \times m$ |
| $(X\theta - Y)$ | $m \times 1$ |
| **R√©sultat** | $(n+1) \times 1$ ‚úì |

### 3.4 R√®gle de mise √† jour compl√®te

$$\boxed{\theta \leftarrow \theta - \frac{\alpha}{m}\, X^\top(X\theta - Y)}$$

---

## 4. Le taux d'apprentissage $\alpha$

### 4.1 R√¥le g√©om√©trique

$\alpha > 0$ contr√¥le la **longueur du pas** dans la direction du gradient. Le gradient ne donne qu'une direction, pas une distance optimale.

### 4.2 Comportements selon la valeur de $\alpha$

**$\alpha$ trop petit :**

$$\theta^{(0)} \;\xrightarrow{\text{petit pas}}\; \theta^{(1)} \;\xrightarrow{\text{petit pas}}\; \theta^{(2)} \;\to\; \cdots \;\to\; \theta^*$$

La convergence est garantie mais **extr√™mement lente**. Il faut un nombre √©lev√© d'it√©rations.

**$\alpha$ bien choisi :**

$$\theta^{(0)} \;\xrightarrow{\text{pas adapt√©}}\; \theta^{(1)} \;\xrightarrow{\text{pas adapt√©}}\; \cdots \;\to\; \theta^*$$

Les pas sont grands mais restent dans la "vall√©e". $J$ diminue r√©guli√®rement √† chaque it√©ration.

**$\alpha$ trop grand :**

$$\theta^{(0)} \;\xrightarrow{\text{grand pas}}\; \theta^{(1)} \;\xleftarrow{\text{grand pas}}\; \theta^{(2)} \;\to\; \cdots$$

On "saute" d'un c√¥t√© √† l'autre du minimum. $J$ peut osciller puis **diverger** vers l'infini.

### 4.3 Condition de convergence (cas quadratique)

Pour une fonction de co√ªt quadratique (comme la MSE en r√©gression lin√©aire), la convergence est garantie si :

$$0 < \alpha < \frac{2}{\lambda_{\max}}$$

o√π $\lambda_{\max}$ est la plus grande valeur propre de la matrice $X^\top X$. En pratique, on choisit souvent :

$$\alpha \approx \frac{1}{\lambda_{\max}}$$

### 4.4 Strat√©gie de s√©lection pratique

En pratique, on teste des valeurs sur une √©chelle logarithmique et on plot $J$ en fonction des it√©rations :

$$\alpha \in \{0.0001,\; 0.001,\; 0.01,\; 0.1,\; 1\}$$

On retient la plus grande valeur pour laquelle $J$ d√©cro√Æt r√©guli√®rement.

---
