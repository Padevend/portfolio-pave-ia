# R√©gression Lin√©aire

> üì¶ Projet source : [ml-from-scratch / regression_lineaire](https://github.com/Padevend/ml-from-scratch/tree/main/regression_lineaire)

---

## 1. Dataset

### 1.1 Repr√©sentation

On consid√®re un dataset $\mathcal{D}$ compos√© de $m$ exemples. Chaque exemple $i$ est un couple $(x^{(i)},\, y^{(i)})$ o√π :

| Symbole | Meaning |
|---|---|
| $x^{(i)} \in \mathbb{R}^n$ | Vecteur de caract√©ristiques de l'exemple $i$ |
| $y^{(i)} \in \mathbb{R}$ | Valeur cible (label) associ√©e |
| $m$ | Nombre total d'exemples |
| $n$ | Nombre de caract√©ristiques |

$$\mathcal{D} = \left\{\, \big(x^{(1)}, y^{(1)}\big),\; \big(x^{(2)}, y^{(2)}\big),\; \dots,\; \big(x^{(m)}, y^{(m)}\big) \,\right\}$$

### 1.2 Convention : ajout du biais dans $x$

Pour simplifier les calculs, on ajoute une colonne constante √©gale √† 1 en premi√®re position de chaque vecteur :

$$x^{(i)} = \begin{pmatrix} 1 \\ x_1^{(i)} \\ x_2^{(i)} \\ \vdots \\ x_n^{(i)} \end{pmatrix} \in \mathbb{R}^{n+1}$$

Le premier √©l√©ment correspond au terme de biais $b$ dans le mod√®le (voir ¬ß2). Le vecteur de param√®tres sera donc de dimension $n+1$.

### 1.3 Matrices $X$ et $Y$

On empile tous les exemples dans une matrice $X$ et un vecteur $Y$ :

$$X = \begin{pmatrix} ‚Äî & (x^{(1)})^\top & ‚Äî \\ ‚Äî & (x^{(2)})^\top & ‚Äî \\ \vdots & \vdots & \vdots \\ ‚Äî & (x^{(m)})^\top & ‚Äî \end{pmatrix} \in \mathbb{R}^{m \times (n+1)}$$

$$Y = \begin{pmatrix} y^{(1)} \\ y^{(2)} \\ \vdots \\ y^{(m)} \end{pmatrix} \in \mathbb{R}^{m}$$

Chaque **ligne** de $X$ est un exemple ; chaque **colonne** (apr√®s la premi√®re) est une caract√©ristique.

---

## 2. Mod√®le ‚Äî $f(x) = ax + b$

### 2.1 Cas univari√© ($n = 1$)

Le mod√®le pr√©dit une valeur $\hat{y}$ √† partir d'une entr√©e scalaire $x$ :

$$\boxed{f(x) = ax + b}$$

| Param√®tre | R√¥le |
|---|---|
| $a \in \mathbb{R}$ | Pente (coefficient directeur) |
| $b \in \mathbb{R}$ | Ordonn√©e √† l'origine (biais) |

### 2.2 G√©n√©ralisation multivari√©e ($n$ caract√©ristiques)

Pour $n > 1$, le mod√®le devient un produit scalaire :

$$f(x) = \underbrace{w_1 x_1 + w_2 x_2 + \cdots + w_n x_n}_{\text{somme pond√©r√©e}} + b$$

En utilisant la convention de ¬ß1.2 (composante 1 ajout√©e), on regroupe tout dans un seul produit scalaire :

$$f(x) = \theta^\top x = \sum_{j=0}^{n} \theta_j\, x_j$$

o√π :

$$\theta = \begin{pmatrix} \theta_0 \\ \theta_1 \\ \vdots \\ \theta_n \end{pmatrix}, \quad x = \begin{pmatrix} 1 \\ x_1 \\ \vdots \\ x_n \end{pmatrix}$$

$\theta_0$ joue le r√¥le de $b$ et $\theta_1, \dots, \theta_n$ jouent le r√¥le de $a$.

### 2.3 Pr√©diction sur le dataset entier

En forme matricielle, les pr√©dictions pour les $m$ exemples sont :

$$\hat{Y} = X\theta \in \mathbb{R}^m$$

$$\hat{Y} = \begin{pmatrix} (x^{(1)})^\top \theta \\ (x^{(2)})^\top \theta \\ \vdots \\ (x^{(m)})^\top \theta \end{pmatrix} = \begin{pmatrix} \hat{y}^{(1)} \\ \hat{y}^{(2)} \\ \vdots \\ \hat{y}^{(m)} \end{pmatrix}$$

---

## 3. Fonction de co√ªt

L'objectif est de trouver $\theta$ qui minimise l'√©cart entre les pr√©dictions $\hat{Y}$ et les valeurs r√©elles $Y$.

### 3.1 Erreur sur un seul exemple

Pour l'exemple $i$, l'erreur (r√©sidu) est :

$$e^{(i)} = \hat{y}^{(i)} - y^{(i)} = \theta^\top x^{(i)} - y^{(i)}$$

### 3.2 Mean Squared Error (MSE)

On utilise l'erreur quadratique moyenne, qui p√©nalise les grands √©carts :

$$\boxed{J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} \left(\theta^\top x^{(i)} - y^{(i)}\right)^2}$$

Le facteur $\frac{1}{2}$ est une convention pratique : il simplifie la d√©riv√©e lors de la descente de gradient (voir ¬ß3.3). Le facteur $\frac{1}{m}$ normalise par rapport au nombre d'exemples.

**Forme vectorielle :**

$$\boxed{J(\theta) = \frac{1}{2m} \left\| X\theta - Y \right\|_2^2 = \frac{1}{2m} (X\theta - Y)^\top (X\theta - Y)}$$

### 3.3 Gradient de $J$

Pour optimiser $\theta$ par descente de gradient, on calcule $\nabla_\theta J$ :

$$\nabla_\theta J = \frac{\partial J}{\partial \theta} = \frac{1}{m} X^\top (X\theta - Y)$$

**D√©rivation d√©taill√©e :**

On pose $R = X\theta - Y$ (vecteur des r√©sidus). Alors :

$$J = \frac{1}{2m} R^\top R$$

$$\frac{\partial J}{\partial \theta} = \frac{1}{2m} \cdot 2\, X^\top R = \frac{1}{m} X^\top (X\theta - Y)$$

On utilise ici la r√®gle $\frac{\partial}{\partial \theta}(R^\top R) = 2 X^\top R$ qui vient du fait que $R$ est lin√©aire en $\theta$.

### 3.4 Mise √† jour par descente de gradient

On r√©p√®te it√©rativement :

$$\boxed{\theta \leftarrow \theta - \alpha \cdot \frac{1}{m} X^\top(X\theta - Y)}$$

o√π $\alpha > 0$ est le **taux d'apprentissage** (learning rate). Il contr√¥le la taille du pas vers le minimum.

| Valeur de $\alpha$ | Comportement |
|---|---|
| Trop petit | Convergence tr√®s lente |
| Bien choisi | Convergence r√©guli√®re vers le minimum |
| Trop grand | Oscillations, divergence possible |

---


## 4. √âcriture en matrice ‚Äî R√©capitulatif

Voici l'ensemble des formules cl√©s sous forme matricielle, dans l'ordre logique du pipeline :

### 4.1 Donn√©es

$$X \in \mathbb{R}^{m \times (n+1)}, \quad Y \in \mathbb{R}^{m}, \quad \theta \in \mathbb{R}^{n+1}$$

### 4.2 Pr√©diction

$$\hat{Y} = X\theta$$

### 4.3 R√©sidus

$$R = \hat{Y} - Y = X\theta - Y$$

### 4.4 Fonction de co√ªt

$$J(\theta) = \frac{1}{2m}\, R^\top R = \frac{1}{2m} \|X\theta - Y\|_2^2$$

### 4.5 Gradient

$$\nabla_\theta J = \frac{1}{m}\, X^\top R = \frac{1}{m}\, X^\top(X\theta - Y)$$

### 4.6 Mise √† jour (gradient descent)

$$\theta \leftarrow \theta - \frac{\alpha}{m}\, X^\top(X\theta - Y)$$

### 4.7 Solution analytique (√©quation normale)

$$\theta^* = (X^\top X)^{-1} X^\top Y$$

---

## 5. Liens & Ressources

 üìÇ Projet GitHub : [ml-from-scratch / regression_lineaire](https://github.com/Padevend/ml-from-scratch/tree/main/regression_lineaire) 
 üìÇ Repo racine : [Padevend / ml-from-scratch](https://github.com/Padevend/ml-from-scratch)